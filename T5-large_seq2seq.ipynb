{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, T5Config, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set paths\n",
    "DATA_PATH = \"data/dataset_sequences_renamed64plus_filtered_dropna_hope5.csv\"\n",
    "MODEL_PATH = \"models/peptide_model\"\n",
    "\n",
    "def clear_cuda_memory():\n",
    "    \"\"\"Clear CUDA memory and garbage collect to free up resources\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "\n",
    "class FocalLossWithSmoothing(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal loss with label smoothing for handling class imbalance\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=2.0, smoothing=0.035, ignore_index=-100):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.smoothing = smoothing\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim() > 2:\n",
    "            input = input.view(-1, input.size(-1))\n",
    "        target = target.view(-1)\n",
    "\n",
    "        mask = target != self.ignore_index\n",
    "        input = input[mask]\n",
    "        target = target[mask]\n",
    "\n",
    "        n_class = input.size(1)\n",
    "\n",
    "        one_hot = torch.zeros_like(input).scatter(1, target.unsqueeze(1), 1)\n",
    "        one_hot = one_hot * (1 - self.smoothing) + (1 - one_hot) * self.smoothing / (n_class - 1)\n",
    "\n",
    "        log_prb = F.log_softmax(input, dim=1)\n",
    "        prb = torch.exp(log_prb)\n",
    "\n",
    "        focal_loss = -(torch.pow(1-prb, self.gamma)) * one_hot * log_prb\n",
    "\n",
    "        return focal_loss.sum(dim=1).mean()\n",
    "\n",
    "\n",
    "class NoisyT5(nn.Module):\n",
    "    \"\"\"\n",
    "    T5 model with added Gaussian noise to improve robustness\n",
    "    \"\"\"\n",
    "    def __init__(self, t5_model, noise_std=0.1):\n",
    "        super().__init__()\n",
    "        self.t5 = t5_model\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, inputs_embeds=None, return_dict=True, **kwargs):\n",
    "        if inputs_embeds is None and input_ids is not None:\n",
    "            inputs_embeds = self.t5.shared(input_ids)\n",
    "\n",
    "        noise = torch.randn_like(inputs_embeds) * self.noise_std\n",
    "        noisy_embeddings = inputs_embeds + noise\n",
    "\n",
    "        return self.t5(\n",
    "            inputs_embeds=noisy_embeddings,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            return_dict=return_dict,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def generate(self, *args, **kwargs):\n",
    "        return self.t5.generate(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingHistory:\n",
    "    \"\"\"Class for tracking and visualizing training metrics\"\"\"\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.epochs = []\n",
    "        self.learning_rates = []\n",
    "        self.fold = None\n",
    "\n",
    "    def add_epoch(self, epoch, train_loss, val_loss, lr):\n",
    "        self.epochs.append(epoch)\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.learning_rates.append(lr)\n",
    "\n",
    "    def set_fold(self, fold):\n",
    "        self.fold = fold\n",
    "\n",
    "    def plot_losses(self, save_path=None):\n",
    "        plt.close('all')\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12))\n",
    "        title_prefix = f\"Fold {self.fold}: \" if self.fold is not None else \"\"\n",
    "\n",
    "        ax1.plot(self.epochs, self.train_losses, label='Training Loss')\n",
    "        ax1.plot(self.epochs, self.val_losses, label='Validation Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title(f'{title_prefix}Training and Validation Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "\n",
    "        ax2.plot(self.epochs, self.learning_rates, label='Learning Rate')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Learning Rate')\n",
    "        ax2.set_title(f'{title_prefix}Learning Rate Schedule')\n",
    "        ax2.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "    def save_history(self, path):\n",
    "        history_dict = {\n",
    "            'fold': self.fold,\n",
    "            'epochs': self.epochs,\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses,\n",
    "            'learning_rates': self.learning_rates\n",
    "        }\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(history_dict, f)\n",
    "\n",
    "\n",
    "class RegularTokenizer:\n",
    "    \"\"\"\n",
    "    Custom tokenizer for protein sequences with special characters\n",
    "    for phosphorylation sites\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.chars = set('ACDEFGHIKLMNPQRSTUVWXY+-?abcdefghijklmnopqrstuvwxyz[]!@#$%^&*<>,.')\n",
    "        self.pad_token = \"<pad>\"\n",
    "        self.unk_token = \"<unk>\"\n",
    "        self.eos_token = \"</s>\"\n",
    "        self.bos_token = \"<s>\"\n",
    "\n",
    "        special_tokens = [self.pad_token, self.unk_token, self.eos_token, self.bos_token]\n",
    "        self.token_to_id = {token: idx for idx, token in enumerate(special_tokens)}\n",
    "        next_idx = len(special_tokens)\n",
    "\n",
    "        for char in sorted(self.chars):\n",
    "            self.token_to_id[char] = next_idx\n",
    "            next_idx += 1\n",
    "\n",
    "        self.id_to_token = {idx: token for token, idx in self.token_to_id.items()}\n",
    "        self.pad_token_id = self.token_to_id[self.pad_token]\n",
    "        self.eos_token_id = self.token_to_id[self.eos_token]\n",
    "        self.bos_token_id = self.token_to_id[self.bos_token]\n",
    "        self.vocab_size = len(self.token_to_id)\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.bos_token_id] + [self.token_to_id.get(c, self.token_to_id[self.unk_token])\n",
    "                                     for c in text] + [self.eos_token_id]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        if torch.is_tensor(ids):\n",
    "            ids = ids.cpu().tolist()\n",
    "        return ''.join([self.id_to_token[id] for id in ids\n",
    "                       if id not in [self.pad_token_id, self.bos_token_id, self.eos_token_id]])\n",
    "\n",
    "    def __call__(self, texts, max_length=136, padding='max_length', truncation=True, return_tensors=None):\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        batch_ids = []\n",
    "        for text in texts:\n",
    "            ids = self.encode(text)\n",
    "            if truncation and len(ids) > max_length:\n",
    "                ids = ids[:max_length-1] + [self.eos_token_id]\n",
    "            if padding == 'max_length':\n",
    "                pad_length = max_length - len(ids)\n",
    "                if pad_length > 0:\n",
    "                    ids = ids + [self.pad_token_id] * pad_length\n",
    "            batch_ids.append(ids)\n",
    "        if return_tensors == \"pt\":\n",
    "            return {\n",
    "                \"input_ids\": torch.tensor(batch_ids),\n",
    "                \"attention_mask\": torch.tensor([[1 if id != self.pad_token_id else 0 for id in ids]\n",
    "                                             for ids in batch_ids])\n",
    "            }\n",
    "        return batch_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeptideDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for peptide sequences with and without phosphorylation markers\n",
    "    \"\"\"\n",
    "    def __init__(self, sequences_no_plus, sequences_with_plus, tokenizer, max_length=136):\n",
    "        self.sequences_no_plus = sequences_no_plus\n",
    "        self.sequences_with_plus = sequences_with_plus\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences_no_plus)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence_no_plus = self.sequences_no_plus[idx]\n",
    "        sequence_with_plus = self.sequences_with_plus[idx]\n",
    "\n",
    "        source_encoding = self.tokenizer(\n",
    "            sequence_no_plus,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        target_encoding = self.tokenizer(\n",
    "            sequence_with_plus,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': source_encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': source_encoding['attention_mask'].squeeze(),\n",
    "            'labels': target_encoding['input_ids'].squeeze()\n",
    "        }\n",
    "\n",
    "\n",
    "def get_optimal_batch_size(model, dataset, device, start_size=2):\n",
    "    \"\"\"\n",
    "    Determine the optimal batch size that fits in GPU memory\n",
    "    \"\"\"\n",
    "    clear_cuda_memory()\n",
    "    batch_size = start_size\n",
    "    while True:\n",
    "        try:\n",
    "            loader = DataLoader(dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=4,\n",
    "                              pin_memory=True)\n",
    "            batch = next(iter(loader))\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            with autocast(device_type='cuda'):\n",
    "                outputs = model(input_ids=input_ids,\n",
    "                              attention_mask=attention_mask,\n",
    "                              labels=labels)\n",
    "            del outputs, input_ids, attention_mask, labels\n",
    "            clear_cuda_memory()\n",
    "            batch_size *= 2\n",
    "        except RuntimeError:\n",
    "            clear_cuda_memory()\n",
    "            return batch_size // 2\n",
    "\n",
    "\n",
    "def check_predictions(model, val_loader, tokenizer, device, num_samples=2):\n",
    "    \"\"\"\n",
    "    Generate predictions on a batch of validation data and print comparison\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    clear_cuda_memory()\n",
    "    with torch.no_grad(), autocast(device_type='cuda'):\n",
    "        batch = next(iter(val_loader))\n",
    "        input_ids = batch['input_ids'][:num_samples].to(device)\n",
    "        attention_mask = batch['attention_mask'][:num_samples].to(device)\n",
    "        labels = batch['labels'][:num_samples]\n",
    "\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=136,\n",
    "            num_beams=5,\n",
    "            length_penalty=1.0,\n",
    "            early_stopping=True,\n",
    "            min_length=111\n",
    "        )\n",
    "\n",
    "        for pred, label in zip(outputs, labels):\n",
    "            pred_text = tokenizer.decode(pred.cpu())\n",
    "            label_text = tokenizer.decode(label)\n",
    "            print(f\"\\nPredicted: {pred_text}\")\n",
    "            print(f\"Actual: {label_text}\")\n",
    "\n",
    "        del outputs, input_ids, attention_mask\n",
    "        clear_cuda_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, tokenizer, device, fold=None, num_epochs=80, save_path=None):\n",
    "    \"\"\"\n",
    "    Train the model with mixed precision, gradient accumulation, and early stopping\n",
    "    \"\"\"\n",
    "    optimizer = AdamW(model.parameters(), lr=4e-5, weight_decay=0.01)\n",
    "    num_training_steps = len(train_loader) * num_epochs * 2\n",
    "    num_warmup_steps = int(num_training_steps * 0.07)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "    criterion = FocalLossWithSmoothing(gamma=2.0, smoothing=0.035)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    history = TrainingHistory()\n",
    "    if fold is not None:\n",
    "        history.set_fold(fold)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 3\n",
    "    no_improvement = 0\n",
    "    accumulation_steps = 2\n",
    "    max_grad_norm = 0.5\n",
    "    noise_iterations = 2\n",
    "    total_steps = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for i, batch in enumerate(tqdm(train_loader)):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            for noise_iter in range(noise_iterations):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with autocast(device_type='cuda'):\n",
    "                    outputs = model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels,\n",
    "                        return_dict=True\n",
    "                    )\n",
    "\n",
    "                    logits = outputs.logits.view(-1, model.t5.config.vocab_size)\n",
    "                    labels_view = labels.view(-1)\n",
    "                    loss = criterion(logits, labels_view) / accumulation_steps\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                total_train_loss += loss.item() * accumulation_steps\n",
    "\n",
    "                del outputs, loss, logits, labels_view\n",
    "                clear_cuda_memory()\n",
    "\n",
    "                if (i * noise_iterations + noise_iter + 1) % accumulation_steps == 0:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    total_steps += 1\n",
    "\n",
    "            del input_ids, attention_mask, labels\n",
    "            clear_cuda_memory()\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                clear_cuda_memory()\n",
    "                model.eval()\n",
    "\n",
    "        total_val_loss = 0\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad(), autocast(device_type='cuda'):\n",
    "            for batch in tqdm(val_loader):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels,\n",
    "                    return_dict=True\n",
    "                )\n",
    "\n",
    "                logits = outputs.logits.view(-1, model.t5.config.vocab_size)\n",
    "                labels_view = labels.view(-1)\n",
    "                loss = criterion(logits, labels_view)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                del outputs, loss, logits, labels_view, input_ids, attention_mask, labels\n",
    "                clear_cuda_memory()\n",
    "\n",
    "            avg_train_loss = total_train_loss / (len(train_loader) * noise_iterations)\n",
    "            avg_val_loss = total_val_loss / len(val_loader)\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "            history.add_epoch(epoch + 1, avg_train_loss, avg_val_loss, current_lr)\n",
    "\n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "            print(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
    "            print(f\"Learning rate: {current_lr:.2e}\")\n",
    "\n",
    "            try:\n",
    "                check_predictions(model, val_loader, tokenizer, device)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in predictions: {e}\")\n",
    "                clear_cuda_memory()\n",
    "                break\n",
    "\n",
    "            if save_path:\n",
    "                fold_suffix = f\"_fold{fold}\" if fold is not None else \"\"\n",
    "                epoch_dir = os.path.join(save_path, f'epoch_{epoch+1}')\n",
    "                os.makedirs(epoch_dir, exist_ok=True)\n",
    "\n",
    "                model.t5.save_pretrained(os.path.join(epoch_dir, f'model{fold_suffix}'))\n",
    "                torch.save(tokenizer, os.path.join(epoch_dir, f'tokenizer{fold_suffix}.pkl'))\n",
    "                history.plot_losses(os.path.join(epoch_dir, f'loss_plot{fold_suffix}.png'))\n",
    "                history.save_history(os.path.join(epoch_dir, f'training_history{fold_suffix}.json'))\n",
    "\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    no_improvement = 0\n",
    "                    best_dir = os.path.join(save_path, 'best_model')\n",
    "                    os.makedirs(best_dir, exist_ok=True)\n",
    "                    model.t5.save_pretrained(os.path.join(best_dir, f'model{fold_suffix}'))\n",
    "                    torch.save(tokenizer, os.path.join(best_dir, f'tokenizer{fold_suffix}.pkl'))\n",
    "                    print(f\"Saved best model with loss: {best_val_loss:.4f}\")\n",
    "                else:\n",
    "                    no_improvement += 1\n",
    "                    if no_improvement >= patience:\n",
    "                        print(\"Early stopping triggered\")\n",
    "                        break\n",
    "\n",
    "            clear_cuda_memory()\n",
    "\n",
    "    return history, best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_cv(df, n_splits=5, base_path=None, start_fold=0, noise_std=0.1):\n",
    "    \"\"\"\n",
    "    Train model with k-fold cross validation\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with sequences data\n",
    "        n_splits: Number of folds\n",
    "        base_path: Path to save models\n",
    "        start_fold: First fold to process (for resuming training)\n",
    "        noise_std: Standard deviation of noise to add\n",
    "        \n",
    "    Returns:\n",
    "        Fold results and best fold index\n",
    "    \"\"\"\n",
    "    clear_cuda_memory()\n",
    "    print(f\"Initializing training from fold {start_fold}/{n_splits}\")\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "    best_fold = None\n",
    "    best_loss = float('inf')\n",
    "    folds = list(kfold.split(df))\n",
    "    \n",
    "    # Set a fixed batch size for all folds\n",
    "    batch_size_test = 32\n",
    "\n",
    "    for fold_idx in range(start_fold, n_splits):\n",
    "        clear_cuda_memory()\n",
    "        train_idx, val_idx = folds[fold_idx]\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing Fold {fold_idx}/{n_splits-1}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        fold_dir = os.path.join(base_path, f'fold_{fold_idx}')\n",
    "        os.makedirs(fold_dir, exist_ok=True)\n",
    "\n",
    "        train_sequences_no_plus = df['Seq_no_plus'].values[train_idx]\n",
    "        train_sequences_with_plus = df['Seq'].values[train_idx]\n",
    "        val_sequences_no_plus = df['Seq_no_plus'].values[val_idx]\n",
    "        val_sequences_with_plus = df['Seq'].values[val_idx]\n",
    "\n",
    "        # Initialize tokenizer and model\n",
    "        tokenizer = RegularTokenizer()\n",
    "        config = T5Config.from_pretrained('t5-large')\n",
    "        config.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "        # Set dropout for regularization\n",
    "        config.dropout_rate = 0.1\n",
    "        config.attention_dropout = 0.1\n",
    "        config.activation_dropout = 0.1\n",
    "        config.classifier_dropout = 0.1\n",
    "\n",
    "        base_model = T5ForConditionalGeneration(config)\n",
    "        model = NoisyT5(base_model, noise_std=noise_std)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "\n",
    "        # Create datasets and data loaders\n",
    "        train_dataset = PeptideDataset(train_sequences_no_plus, train_sequences_with_plus, tokenizer)\n",
    "        val_dataset = PeptideDataset(val_sequences_no_plus, val_sequences_with_plus, tokenizer)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size_test, num_workers=4, \n",
    "                                 pin_memory=True, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size_test, num_workers=4, \n",
    "                               pin_memory=True)\n",
    "\n",
    "        # Train the model\n",
    "        history, val_loss = train_model(model, train_loader, val_loader, tokenizer, \n",
    "                                       device, fold=fold_idx, num_epochs=80, save_path=fold_dir)\n",
    "\n",
    "        fold_results.append({\n",
    "            'fold': fold_idx,\n",
    "            'history': history,\n",
    "            'val_loss': val_loss\n",
    "        })\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_fold = fold_idx\n",
    "\n",
    "        # Clean up memory\n",
    "        del model, train_loader, val_loader, train_dataset, val_dataset\n",
    "        clear_cuda_memory()\n",
    "\n",
    "    print(f\"\\nBest performing fold: {best_fold} with validation loss: {best_loss:.4f}\")\n",
    "    return fold_results, best_fold\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to load data and start training\n",
    "    \"\"\"\n",
    "    clear_cuda_memory()\n",
    "    print(\"Loading dataset from:\", DATA_PATH)\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(f\"Dataset loaded successfully: {len(df)} rows\")\n",
    "\n",
    "    # Create a timestamp folder for this training run\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    results_dir = os.path.join(MODEL_PATH, f'training_run_{timestamp}')\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    print(f\"Results will be saved in: {results_dir}\")\n",
    "\n",
    "    # Set noise standard deviation\n",
    "    noise_std = 0.1\n",
    "    print(f\"Training with noise standard deviation: {noise_std}\")\n",
    "\n",
    "    # Start training with 5-fold cross validation\n",
    "    fold_results, best_fold = train_with_cv(df, n_splits=5, base_path=results_dir, \n",
    "                                           start_fold=0, noise_std=noise_std)\n",
    "\n",
    "    print(\"\\nTraining completed!\")\n",
    "    print(f\"Results saved in {results_dir}\")\n",
    "    clear_cuda_memory()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
